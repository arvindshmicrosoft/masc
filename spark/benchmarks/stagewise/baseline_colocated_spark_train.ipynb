{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Spark Model for AI on Accumulo\n",
    "\n",
    "Before running this notebook, please\n",
    "* create and activate a conda environment with Apache Toree installed\n",
    "* build a shaded jar with Accumulo 2.0.0 and Spark 2.4.3 included\n",
    "* clean yarn cache on the cluster due to previous runs (optoinal but recommended)\n",
    "* run commands like the following to install Jupyter toree kernel\n",
    "```\n",
    "# Replace the jar file path based on your situation\n",
    "JAR=\"file:///home/rba1/chenhui/build_jar/target/accumulo-spark-shaded.jar\"\n",
    "jupyter toree install \\\n",
    "    --replace \\\n",
    "    --user \\\n",
    "    --kernel_name=accumulo \\\n",
    "    --spark_home=${SPARK_HOME} \\\n",
    "    --spark_opts=\"--master yarn --jars $JAR \\\n",
    "        --packages com.microsoft.ml.spark:mmlspark_2.11:0.18.1 \\\n",
    "        --driver-memory 16g \\\n",
    "        --executor-memory 12g \\\n",
    "        --driver-cores 4 \\\n",
    "        --executor-cores 4 \\\n",
    "        --num-executors 128\"\n",
    "```\n",
    "\n",
    "We train the model on duplicated twitter data with a target size and evaluate the inferencing speed on the same dataset. Besides, we evaluate the model accuracy by calculating AUC on a separate test set which is manually labeled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version 2.4.3\n",
      "Scala version 2.11.12\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(spark.kryo.classesToRegister,org.apache.accumulo.core.data.Key,org.apache.accumulo.core.data.Value,java.util.Properties)\n",
      "(spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS,32nodecluster-0)\n",
      "(spark.driver.port,38805)\n",
      "(spark.driver.host,32nodecluster-0)\n",
      "(spark.eventLog.enabled,true)\n",
      "(spark.driver.appUIAddress,http://32nodecluster-0:4040)\n",
      "(spark.repl.local.jars,file:///home/rba1/webscale-ai-test/target/accumulo-spark-shaded.jar,file:///home/rba1/.ivy2/jars/com.microsoft.ml.spark_mmlspark_2.11-0.18.1.jar,file:///home/rba1/.ivy2/jars/org.scalactic_scalactic_2.11-3.0.5.jar,file:///home/rba1/.ivy2/jars/org.scalatest_scalatest_2.11-3.0.5.jar,file:///home/rba1/.ivy2/jars/io.spray_spray-json_2.11-1.3.2.jar,file:///home/rba1/.ivy2/jars/com.microsoft.cntk_cntk-2.4.jar,file:///home/rba1/.ivy2/jars/org.openpnp_opencv-3.2.0-1.jar,file:///home/rba1/.ivy2/jars/com.jcraft_jsch-0.1.54.jar,file:///home/rba1/.ivy2/jars/org.apache.httpcomponents_httpclient-4.5.6.jar,file:///home/rba1/.ivy2/jars/com.microsoft.ml.lightgbm_lightgbmlib-2.2.350.jar,file:///home/rba1/.ivy2/jars/com.github.vowpalwabbit_vw-jni-8.7.0.2.jar,file:///home/rba1/.ivy2/jars/org.scala-lang_scala-reflect-2.11.12.jar,file:///home/rba1/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.6.jar,file:///home/rba1/.ivy2/jars/org.apache.httpcomponents_httpcore-4.4.10.jar,file:///home/rba1/.ivy2/jars/commons-logging_commons-logging-1.2.jar,file:///home/rba1/.ivy2/jars/commons-codec_commons-codec-1.10.jar)\n",
      "(spark.repl.class.outputDir,/tmp/spark-6f33702d-310e-41d1-b9bd-20371ed112a2/repl-e8671dac-2ef3-40a2-9124-e7244a64d2ac)\n",
      "(spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES,http://32nodecluster-0:8088/proxy/application_1568922621130_0019)\n",
      "(spark.executor.cores,4)\n",
      "(spark.eventLog.dir,hdfs://32nodecluster/spark/history)\n",
      "(spark.yarn.secondary.jars,accumulo-spark-shaded.jar,com.microsoft.ml.spark_mmlspark_2.11-0.18.1.jar,org.scalactic_scalactic_2.11-3.0.5.jar,org.scalatest_scalatest_2.11-3.0.5.jar,io.spray_spray-json_2.11-1.3.2.jar,com.microsoft.cntk_cntk-2.4.jar,org.openpnp_opencv-3.2.0-1.jar,com.jcraft_jsch-0.1.54.jar,org.apache.httpcomponents_httpclient-4.5.6.jar,com.microsoft.ml.lightgbm_lightgbmlib-2.2.350.jar,com.github.vowpalwabbit_vw-jni-8.7.0.2.jar,org.scala-lang_scala-reflect-2.11.12.jar,org.scala-lang.modules_scala-xml_2.11-1.0.6.jar,org.apache.httpcomponents_httpcore-4.4.10.jar,commons-logging_commons-logging-1.2.jar,commons-codec_commons-codec-1.10.jar)\n",
      "(spark.serializer,org.apache.spark.serializer.KryoSerializer)\n",
      "(spark.yarn.historyServer.address,32nodecluster-0:18080)\n",
      "(spark.submit.deployMode,client)\n",
      "(spark.ui.filters,org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter)\n",
      "(spark.yarn.dist.jars,file:///home/rba1/webscale-ai-test/target/accumulo-spark-shaded.jar,file:///home/rba1/.ivy2/jars/com.microsoft.ml.spark_mmlspark_2.11-0.18.1.jar,file:///home/rba1/.ivy2/jars/org.scalactic_scalactic_2.11-3.0.5.jar,file:///home/rba1/.ivy2/jars/org.scalatest_scalatest_2.11-3.0.5.jar,file:///home/rba1/.ivy2/jars/io.spray_spray-json_2.11-1.3.2.jar,file:///home/rba1/.ivy2/jars/com.microsoft.cntk_cntk-2.4.jar,file:///home/rba1/.ivy2/jars/org.openpnp_opencv-3.2.0-1.jar,file:///home/rba1/.ivy2/jars/com.jcraft_jsch-0.1.54.jar,file:///home/rba1/.ivy2/jars/org.apache.httpcomponents_httpclient-4.5.6.jar,file:///home/rba1/.ivy2/jars/com.microsoft.ml.lightgbm_lightgbmlib-2.2.350.jar,file:///home/rba1/.ivy2/jars/com.github.vowpalwabbit_vw-jni-8.7.0.2.jar,file:///home/rba1/.ivy2/jars/org.scala-lang_scala-reflect-2.11.12.jar,file:///home/rba1/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.6.jar,file:///home/rba1/.ivy2/jars/org.apache.httpcomponents_httpcore-4.4.10.jar,file:///home/rba1/.ivy2/jars/commons-logging_commons-logging-1.2.jar,file:///home/rba1/.ivy2/jars/commons-codec_commons-codec-1.10.jar)\n",
      "(spark.history.fs.logDirectory,hdfs://32nodecluster/spark/history)\n",
      "(spark.yarn.am.memory,12g)\n",
      "(spark.app.name,TwitterSentimentClassification)\n",
      "(spark.ui.proxyBase,/proxy/application_1568922621130_0018)\n",
      "(spark.jars,file:/home/rba1/.local/share/jupyter/kernels/accumulo_scala/lib/toree-assembly-0.3.0-incubating.jar)\n",
      "(spark.executor.id,driver)\n",
      "(spark.driver.memory,16g)\n",
      "(spark.executor.instances,128)\n",
      "(spark.executor.memory,12g)\n",
      "(spark.master,yarn)\n",
      "(spark.yarn.am.cores,4)\n",
      "(spark.app.id,application_1568922621130_0019)\n",
      "(spark.driver.maxResultSize,3g)\n",
      "(spark.repl.class.uri,spark://32nodecluster-0:38805/classes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "conf = org.apache.spark.SparkConf@216c2dbb\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.SparkConf@216c2dbb"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.util.Properties\n",
    "\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.accumulo.core.data.{Key, Value}\n",
    "\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "\n",
    "// Stop existing spark context and create new one\n",
    "sc.stop()\n",
    "\n",
    "val conf = new SparkConf()\n",
    "conf.setAppName(\"TwitterSentimentClassification\")\n",
    "// KryoSerializer is needed for serializing Accumulo Key when partitioning data for bulk import\n",
    "conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "conf.registerKryoClasses(Array(classOf[Key], classOf[Value], classOf[Properties]))\n",
    "conf.set(\"spark.driver.maxResultSize\", \"3g\")\n",
    "\n",
    "new SparkContext(conf)\n",
    "\n",
    "println(\"Spark version %s\".format(sc.version))\n",
    "println(\"Scala %s\".format(util.Properties.versionString))\n",
    "println\n",
    "sc.getConf.getAll.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.nio.ByteBuffer\n",
    "import java.util.{ArrayList, Collections, List}\n",
    "\n",
    "import org.apache.accumulo.core.client.{Accumulo, AccumuloClient, BatchWriter, MutationsRejectedException}\n",
    "import org.apache.accumulo.core.data.{Key, Mutation, Value}\n",
    "import org.apache.accumulo.hadoop.mapreduce.{AccumuloFileOutputFormat, AccumuloInputFormat}\n",
    "import org.apache.hadoop.conf.Configuration\n",
    "import org.apache.hadoop.fs.{FileSystem, Path}\n",
    "import org.apache.hadoop.mapreduce.Job\n",
    "import org.apache.spark.Partitioner\n",
    "import org.apache.spark.sql.{DataFrame, Row, SparkSession}\n",
    "import org.apache.spark.sql.functions.{expr, first, max, min, rand, when, lit, col, explode, concat, array}\n",
    "import org.apache.spark.sql.types.{LongType, DoubleType, StringType, StructField, StructType}\n",
    "\n",
    "import scala.collection.mutable.ArrayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PROPS_PATH = ./accumulo-client.properties\n",
       "data_copies = Map(1G -> 5, 10G -> 44, 100G -> 440, 1T -> 4501)\n",
       "DATA_SIZE = 1G\n",
       "N_COPIES = 5\n",
       "TABLE_NAME = twitter1G\n",
       "N_SPLITS = 512\n",
       "WRITE_DUP_DF = false\n",
       "N_PARTS = 128\n",
       "FEATURIZER = token_counts\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "token_counts"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// client property file path\n",
    "val PROPS_PATH = \"./accumulo-client.properties\"\n",
    "// number of data copies based on 233Mb initial data\n",
    "val data_copies = Map(\"1G\" -> 5,  \"10G\" -> 44, \"100G\" -> 440, \"1T\" -> 4501)\n",
    "val DATA_SIZE = \"1G\" //\"1G\"\n",
    "val N_COPIES = data_copies(DATA_SIZE)\n",
    "val TABLE_NAME = \"twitter\".concat(DATA_SIZE)\n",
    "val N_SPLITS = 512\n",
    "// test writing the duplicated data or not\n",
    "val WRITE_DUP_DF = true\n",
    "// number of partitions used in dfToTable()\n",
    "val N_PARTS = conf.get(\"spark.executor.instances\").toInt\n",
    "// type of featurizer \n",
    "val FEATURIZER = \"token_counts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "addSplits: (spark: org.apache.spark.sql.SparkSession, n_splits: Int, client: String, table: String)Unit\n",
       "getSplits: (spark: org.apache.spark.sql.SparkSession, client: String, table: String)Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def addSplits(spark: SparkSession, n_splits: Int, client: String, table: String): Unit = {\n",
    "    import scala.collection.JavaConverters._\n",
    "    import java.util.TreeSet\n",
    "    import org.apache.hadoop.io.Text\n",
    "    val splits_list = Range(0, n_splits).map(x => new Text(\"%04d\".format(x)))\n",
    "    val splits = new TreeSet(splits_list.asJava)\n",
    "    val props = Accumulo.newClientProperties().from(client).build()\n",
    "    val cl = Accumulo.newClient().from(props).build()\n",
    "    cl.tableOperations().addSplits(table, splits)\n",
    "    cl.close()\n",
    "}\n",
    "\n",
    "def getSplits(spark: SparkSession, client: String, table: String): Unit = {\n",
    "    val props = Accumulo.newClientProperties().from(client).build()\n",
    "    val cl = Accumulo.newClient().from(props).build()\n",
    "    println(cl.tableOperations().listSplits(table))\n",
    "    cl.close()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tableToDf: (spark: org.apache.spark.sql.SparkSession, client: String, table: String, colFs: String, timer: Boolean)(org.apache.spark.sql.DataFrame, Double, Double)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/** Load Spark DataFrame from Accumulo table\n",
    " *\n",
    " *  @param spark   Spark session\n",
    " *  @param client  Accumulo client properties filepath\n",
    " *  @param table   Accumulo table name\n",
    " *\n",
    " *  @return DataFrame\n",
    " */\n",
    "def tableToDf(spark: SparkSession, client: String, table: String, colFs: String = \"\", \n",
    "              timer: Boolean = false): (DataFrame, Double, Double) = {\n",
    "    var t0 = System.nanoTime()\n",
    "    import spark.implicits._\n",
    "    val props = Accumulo.newClientProperties().from(client).build()\n",
    "\n",
    "    val cl = Accumulo.newClient().from(props).build()\n",
    "    if (!cl.tableOperations().exists(table)) {\n",
    "        throw new IllegalArgumentException(\"Table %s does not exist.\".format(table))\n",
    "    }\n",
    "    cl.close()\n",
    "\n",
    "    val sc = spark.sparkContext\n",
    "    val job = Job.getInstance()\n",
    "\n",
    "    AccumuloInputFormat.configure().clientProperties(props).table(table).store(job)\n",
    "    \n",
    "    val rawRDD = sc.newAPIHadoopRDD(\n",
    "        job.getConfiguration,\n",
    "        classOf[AccumuloInputFormat],\n",
    "        classOf[Key],\n",
    "        classOf[Value]\n",
    "    )\n",
    "\n",
    "    var rawRDD_time = 0.0\n",
    "    if (timer) {\n",
    "        rawRDD.cache().count()\n",
    "        var t1 = System.nanoTime()\n",
    "        rawRDD_time = (t1 - t0)*1e-9\n",
    "        println(\"Time until rawRDD: \" + rawRDD_time + \"s\")\n",
    "    } \n",
    "\n",
    "    import spark.implicits._\n",
    "    val rawDF = rawRDD.mapPartitions( partition => {\n",
    "        partition.map({\n",
    "            case (k, v) => (\n",
    "                k.getRow().toString,\n",
    "                k.getColumnFamily().toString,  // To get column qualifier, add k.getColumnQualifier().toString\n",
    "                v.toString\n",
    "            )\n",
    "        })\n",
    "    }).toDF(\"id\", \"colF\", \"value\")\n",
    "    \n",
    "    if (timer) {\n",
    "        rawDF.cache().count()\n",
    "        var t2 = System.nanoTime()\n",
    "        println(\"Time until rawDF: \" + (t2 - t0)*1e-9 + \"s\")\n",
    "    } \n",
    "\n",
    "    var pivotDF = rawDF\n",
    "    if (colFs.length > 0) {\n",
    "        pivotDF = rawDF.groupBy(\"id\").pivot(\"colF\", colFs.split(\"\\\\s*,\\\\s*\").toSeq).agg(first($\"value\"))\n",
    "    }\n",
    "    else {\n",
    "        pivotDF = rawDF.groupBy(\"id\").pivot(\"colF\").agg(first($\"value\"))\n",
    "    }\n",
    "    \n",
    "    var trans_time = 0.0\n",
    "    if (timer) {\n",
    "        pivotDF.cache().count()\n",
    "        var t3 = System.nanoTime() \n",
    "        var pivotDF_time = (t3 - t0)*1e-9\n",
    "        trans_time = pivotDF_time - rawRDD_time\n",
    "        println(\"Time until pivotDF: \" + pivotDF_time + \"s\")\n",
    "        println(\"Data transform time: \" + trans_time + \"s\")\n",
    "    } \n",
    "    rawRDD.unpersist()\n",
    "    rawDF.unpersist()\n",
    "    \n",
    "    (pivotDF, rawRDD_time, trans_time)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfToTable: (spark: org.apache.spark.sql.SparkSession, df: org.apache.spark.sql.DataFrame, numParts: Int, client: String, table: String, defaultFS: String, writeMode: String, insertMode: String)Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/** Inject Spark DataFrame into Accumulo table.\n",
    " *  DataFrame should include a column named \"id\" (type Long) which will be used as the table's row id.\n",
    " *\n",
    " *  @param spark       Spark session\n",
    " *  @param df          Spark DataFrame\n",
    " *  @param numParts    Number of partitions to use. Should be NUM-EXECUTORS * n for the best performance\n",
    " *  @param client      Accumulo client properties filepath\n",
    " *  @param table       Accumulo table name\n",
    " *  @param defaultFS   Hadoop FileSystem url for \"bulk\" import \n",
    " *  @param writeMode   Write mode - \"batch\" or \"bulk\"\n",
    " *  @param insertMode  Insert mode - \"replace\" (delete existing table if exist),\n",
    " *                     \"overwrite\" or \"into\" (ignore existing keys). \n",
    " */\n",
    "def dfToTable(spark: SparkSession, df: DataFrame, numParts: Int, client: String, table: String,\n",
    "              defaultFS: String = \"\", writeMode: String = \"batch\", insertMode: String = \"replace\"): Unit = {\n",
    "    // Check if df contains \"id\" column\n",
    "    if (!(df.columns contains \"id\")) {\n",
    "        throw new IllegalArgumentException(\"Input DataFrame should have 'id' column.\")\n",
    "    }\n",
    "\n",
    "    import spark.implicits._\n",
    "    val sc = spark.sparkContext\n",
    "\n",
    "    // Create table (TODO other insert modes...)\n",
    "    val props = Accumulo.newClientProperties().from(client).build()\n",
    "    val cl = Accumulo.newClient().from(props).build()\n",
    "    if (insertMode == \"replace\") {\n",
    "        // Replace the existing table (if any)\n",
    "        if (cl.tableOperations().exists(table)) cl.tableOperations().delete(table)\n",
    "        cl.tableOperations().create(table)\n",
    "    }\n",
    "    \n",
    "    // Pre-split the table \n",
    "    addSplits(spark, N_SPLITS, client, table)\n",
    "\n",
    "    // Make new schema without \"id\" column\n",
    "    val schema = df.schema.filter(_.name != \"id\")\n",
    "\n",
    "    // Convert columns to String except \"id\". Double type \"id\" will be used by bulk import's rangePartition\n",
    "    val dfStr = schema.filter(_.dataType != StringType).foldLeft(df) {\n",
    "        case (df, col) => df.withColumn(col.name, df(col.name).cast(StringType))\n",
    "    }\n",
    "\n",
    "    if (writeMode == \"batch\") {\n",
    "        val bcSchema = sc.broadcast(schema)\n",
    "        val dfParted = dfStr.repartition(numParts, $\"id\") \n",
    "        dfParted.foreachPartition { partition =>\n",
    "            // Intentionally created an Accumulo client for each partition to avoid attempting to\n",
    "            // serialize it and send it to each remote process.\n",
    "            var cl = None: Option[AccumuloClient]\n",
    "            var bw = None: Option[BatchWriter]\n",
    "            try {\n",
    "                cl = Some(Accumulo.newClient().from(props).build())\n",
    "                bw = Some(cl.get.createBatchWriter(table))\n",
    "                // TODO Maybe use partition.grouped(1000).foreach\n",
    "                partition.foreach { record =>\n",
    "                    val m = new Mutation(record.getAs[Long](\"id\").toString)\n",
    "                    bcSchema.value.foreach { s =>\n",
    "                        // at() has been introduced since 2.0.0.\n",
    "                        // Add .visibility(v).timestamp(t) if needed\n",
    "                        m.at().family(s.name).qualifier(\"\").put(record.getAs[String](s.name))\n",
    "                    }\n",
    "                    try {\n",
    "                        bw.get.addMutation(m)\n",
    "                    }\n",
    "                    catch {\n",
    "                        case e: MutationsRejectedException => e.printStackTrace()\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            finally {\n",
    "                if (bw.isDefined) bw.get.close()\n",
    "                if (cl.isDefined) cl.get.close()\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    else {\n",
    "        throw new IllegalArgumentException(\"Write mode should be either 'batch' or 'bulk'\")\n",
    "    }\n",
    "\n",
    "    cl.close()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tables: (spark: org.apache.spark.sql.SparkSession, client: String)Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tables(spark: SparkSession, client: String): Unit = {\n",
    "    val props = Accumulo.newClientProperties().from(client).build()\n",
    "    val cl = Accumulo.newClient().from(props).build()\n",
    "    println(cl.tableOperations().list())\n",
    "    cl.close()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "duplicateDf: (inDf: org.apache.spark.sql.DataFrame, nDup: Int, onlyID: Boolean)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def duplicateDf(inDf: DataFrame, nDup: Int, onlyID: Boolean = false) : DataFrame = {    \n",
    "    val result = inDf\n",
    "        .withColumn(\"dup_idx\", explode(array((1 until (nDup+1)).map(lit): _*)))\n",
    "        .withColumn(\"id\", concat(col(\"id\"), lit(\"_\"), col(\"dup_idx\")))\n",
    "    if (onlyID) {\n",
    "        return result.select(\"id\")\n",
    "    } else {\n",
    "        return result.select(\"id\", \"sentiment\", \"date\", \"query_string\", \"user\", \"text\")\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip writing the duplicated dataframe into Accumulo!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df_dup: org.apache.spark.sql.DataFrame = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Write duplicated twitter data into Accumulo\n",
    "var df_dup: DataFrame = _\n",
    "if (WRITE_DUP_DF) {\n",
    "    // define data schema\n",
    "    val schema = StructType(Array(\n",
    "        StructField(\"sentiment\", DoubleType),\n",
    "        StructField(\"id\", StringType),\n",
    "        StructField(\"date\", StringType),\n",
    "        StructField(\"query_string\", StringType),\n",
    "        StructField(\"user\", StringType),\n",
    "        StructField(\"text\", StringType)\n",
    "    ))\n",
    "    // need to upload data to hdfs first via \n",
    "    // hdfs dfs -put /home/rba1/chenhui/accumulo_ml_baseline/data/sentiment140.csv sentiment140.csv\n",
    "    val file_path = \"sentiment140_prefix.csv\"\n",
    "    val df = spark.read.format(\"csv\").schema(schema).load(file_path)\n",
    "    \n",
    "    df_dup = duplicateDf(df, N_COPIES)\n",
    "    df_dup.cache().count()\n",
    "    \n",
    "    var t0 = System.nanoTime()\n",
    "    dfToTable(\n",
    "        spark,\n",
    "        df=df_dup,\n",
    "        numParts=N_PARTS,\n",
    "        client=PROPS_PATH,\n",
    "        table=TABLE_NAME\n",
    "    )\n",
    "    var t1 = System.nanoTime()\n",
    "    println(\"Elapsed time: \" + (t1 - t0)*1e-9 + \"s\")\n",
    "    \n",
    "    df_dup.unpersist()\n",
    "} else {\n",
    "    println(\"Skip writing the duplicated dataframe into Accumulo!\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from Accumulo...\n",
      "Time until rawRDD: 8.922607352s\n",
      "Time until rawDF: 14.490108339s\n",
      "Time until pivotDF: 23.639355363s\n",
      "Data transform time: 14.716748011s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df_all = [id: string, date: string ... 4 more fields]\n",
       "read_time = 8.922607352\n",
       "trans_time = 14.716748011\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "14.716748011"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Read training data from Accumulo\n",
    "println(\"Reading data from Accumulo...\")\n",
    "val (df_all, \n",
    "     read_time, \n",
    "     trans_time) = \n",
    "     tableToDf(spark, \n",
    "               client=PROPS_PATH, \n",
    "               table=TABLE_NAME,\n",
    "               timer=true\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+------------+---------+--------------------+--------------+\n",
      "|               id|                date|query_string|sentiment|                text|          user|\n",
      "+-----------------+--------------------+------------+---------+--------------------+--------------+\n",
      "|0000_1467833736_2|Mon Apr 06 22:25:...|    NO_QUERY|      0.0|No new csi tonigh...|  MagicalMason|\n",
      "|0000_1467856044_3|Mon Apr 06 22:31:...|    NO_QUERY|      0.0|Crazy wind today ...|   EcoTravelTV|\n",
      "|0000_1467901839_5|Mon Apr 06 22:43:...|    NO_QUERY|      0.0|@CarVin1 lol they...|SummerJSanders|\n",
      "|0000_1467917499_1|Mon Apr 06 22:48:...|    NO_QUERY|      0.0|@RandomlyNat Jeez...| VictoriaBahar|\n",
      "|0000_1467931839_2|Mon Apr 06 22:52:...|    NO_QUERY|      0.0|@margaretcho what...|       dmurda6|\n",
      "|0000_1467945704_2|Mon Apr 06 22:56:...|    NO_QUERY|      0.0|anyone else havin...|      amfairie|\n",
      "|0000_1467987736_2|Mon Apr 06 23:08:...|    NO_QUERY|      0.0|   @polhillian YUP. |       roboppy|\n",
      "|0000_1468006195_1|Mon Apr 06 23:13:...|    NO_QUERY|      0.0|   Sliced my finger |     kristie__|\n",
      "|0000_1468013020_3|Mon Apr 06 23:15:...|    NO_QUERY|      0.0|has a mild left i...|    melvinchia|\n",
      "|0000_1468018399_1|Mon Apr 06 23:17:...|    NO_QUERY|      0.0|Okay, so.. STILL ...|     murielara|\n",
      "+-----------------+--------------------+------------+---------+--------------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_all.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num train samples = 8000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "train_df = [label: double, text: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[label: double, text: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// df_all only contains positive or negative tweets\n",
    "// df_all.select(\"sentiment\").distinct().collect() returns Array([0.0], [4.0])\n",
    "var train_df = df_all.orderBy(rand()) // Randomly permute the data for online training\n",
    "                     .withColumn(\"sentiment2\", 'sentiment.cast(\"Int\"))\n",
    "                     .select('sentiment2 as 'label, 'text as 'text)\n",
    "                     .withColumn(\"label\", when('label > 0, 1.0D).otherwise(0.0D))\n",
    "\n",
    "if ((N_PARTS < 128) && (N_COPIES >= data_copies(\"100G\"))) {\n",
    "    train_df = train_df.repartition(8192)\n",
    "}\n",
    "\n",
    "println(\"Num train samples = %s\".format(train_df.cache().count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.{CountVectorizer, RegexTokenizer}\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer = regexTok_95065a4cc274\n",
       "countVectorizer = cntVec_2ec09e7f3163\n",
       "lr = logreg_5f28d787394f\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "logreg_5f28d787394f"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tokenizer = new RegexTokenizer()\n",
    "  .setGaps(false)\n",
    "  .setPattern(\"\\\\p{L}+\")\n",
    "  .setInputCol(\"text\")\n",
    "  .setOutputCol(\"words\")\n",
    "\n",
    "val countVectorizer = new CountVectorizer()\n",
    "  .setInputCol(\"words\")\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "val lr = new LogisticRegression()\n",
    "  .setMaxIter(1)\n",
    "  .setRegParam(0.2)\n",
    "  .setElasticNetParam(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to fit FE pipeline: 5.442636145000001s\n",
      "Time to get train_fea: 8.601079914000001s\n",
      "Total time: 14.043716059000001s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t0 = 341791514517341\n",
       "feat_eng_pipeline = pipeline_1b109d72b2f1\n",
       "featEng = pipeline_1b109d72b2f1\n",
       "t1 = 341796957153486\n",
       "train_fea = [label: double, text: string ... 2 more fields]\n",
       "t2 = 341805558233400\n",
       "feat_eng_time = 8.601079914000001\n",
       "feat_eng_time_with_fit = 14.043716059000001\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[label: double, text: string]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Evaluate model training speed - feature engineering\n",
    "var t0 = System.nanoTime()\n",
    "\n",
    "val feat_eng_pipeline = new Pipeline().setStages(Array(tokenizer, countVectorizer))\n",
    "val featEng = feat_eng_pipeline.fit(train_df)\n",
    "\n",
    "var t1 = System.nanoTime()\n",
    "\n",
    "var train_fea = featEng.transform(train_df)\n",
    "train_fea.cache().count()\n",
    "\n",
    "var t2 = System.nanoTime()\n",
    "val feat_eng_time = (t2 - t1)*1e-9\n",
    "val feat_eng_time_with_fit = (t2 - t0)*1e-9\n",
    "println(\"Time to fit FE pipeline: \" + (t1 - t0)*1e-9 + \"s\")\n",
    "println(\"Time to get train_fea: \" + feat_eng_time + \"s\")\n",
    "println(\"Total time: \" + feat_eng_time_with_fit + \"s\")\n",
    "train_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train lr model: 7.546550775s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t0 = 341807773979052\n",
       "lrModel = LogisticRegressionModel: uid = logreg_5f28d787394f, numClasses = 2, numFeatures = 262144\n",
       "t1 = 341815320529827\n",
       "train_time = 7.546550775\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[label: double, text: string ... 2 more fields]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Evaluate model training speed - model training\n",
    "if ((N_PARTS < 128) && (N_COPIES >= data_copies(\"100G\"))) {\n",
    "    train_fea = train_fea.repartition(8192)\n",
    "    train_fea.cache().count()\n",
    "}\n",
    "\n",
    "var t0 = System.nanoTime()\n",
    "\n",
    "val lrModel = lr.fit(train_fea)\n",
    "\n",
    "var t1 = System.nanoTime()\n",
    "val train_time = (t1 - t0)*1e-9\n",
    "println(\"Time to train lr model: \" + train_time + \"s\")\n",
    "train_fea.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Save feature engineering pipeline and model to hdfs\n",
    "featEng.write.overwrite().save(\"./model/lrFeatEng_\".concat(DATA_SIZE))\n",
    "lrModel.write.overwrite().save(\"./model/lrModel_\".concat(DATA_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "columns = List(id, read_time, trans_time, feat_eng_time, feat_eng_time_with_fit, train_time)\n",
       "trial_id = application_1568922621130_0019_2019-09-23T18:39:05.992\n",
       "results = List((application_1568922621130_0019_2019-09-23T18:39:05.992,8.922607352,14.716748011,8.601079914000001,14.043716059000001,7.546550775))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "List((application_1568922621130_0019_2019-09-23T18:39:05.992,8.922607352,14.716748011,8.601079914000001,14.043716059000001,7.546550775))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.time.LocalDateTime\n",
    "val columns = Seq(\"id\", \"read_time\", \"trans_time\", \"feat_eng_time\", \"feat_eng_time_with_fit\", \"train_time\")\n",
    "val trial_id = sc.applicationId + \"_\" + LocalDateTime.now()\n",
    "val results = Seq((trial_id, \n",
    "                   read_time, \n",
    "                   trans_time,\n",
    "                   feat_eng_time, \n",
    "                   feat_eng_time_with_fit,\n",
    "                   train_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+------------+-----------------+----------------------+-----------+\n",
      "|                  id|  read_time|  trans_time|    feat_eng_time|feat_eng_time_with_fit| train_time|\n",
      "+--------------------+-----------+------------+-----------------+----------------------+-----------+\n",
      "|application_15689...|8.922607352|14.716748011|8.601079914000001|    14.043716059000001|7.546550775|\n",
      "+--------------------+-----------+------------+-----------------+----------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df_results = [id: string, read_time: double ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: string, read_time: double ... 4 more fields]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_results = results.toDF(columns: _*)\n",
    "df_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "results_tablename = lr_train_results_128_executors_token_counts_1G\n",
       "results_filename = lr_train_results_128_executors_token_counts_1G.csv\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lr_train_results_128_executors_token_counts_1G.csv"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val results_tablename = \"lr_train_results_%s_executors_%s_%s\" format(N_PARTS, FEATURIZER, DATA_SIZE)\n",
    "val results_filename = results_tablename + \".csv\"\n",
    "\n",
    "// Write results to csv file\n",
    "df_results.write.format(\"com.databricks.spark.csv\").mode(\"overwrite\").save(\".results/\" + results_filename)\n",
    "\n",
    "// Write results to Accumulo\n",
    "dfToTable(\n",
    "    spark,\n",
    "    df=df_results,\n",
    "    numParts=1,\n",
    "    client=PROPS_PATH,\n",
    "    table=results_tablename\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accumulo - Scala",
   "language": "scala",
   "name": "accumulo_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
