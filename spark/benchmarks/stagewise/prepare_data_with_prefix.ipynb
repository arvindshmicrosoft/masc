{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Twitter Data with Row ID Prefixes\n",
    "\n",
    "This notebook downloads Sentiment140 twitter data and appends a prefix to each row ID.The prefix is a zero-padded integer ranging from 0 to 511. Later we will create duplicated twitter data based on the data with row ID prefixes for our benchmark tests. With the prefixes, we can easily split Accumulo tables used for holding replicated twitter data and speed up the process of writing data into these tables.\n",
    "\n",
    "Note that you need to run this notebook in Python 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from zipfile import ZipFile\n",
    "import urllib.request\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to download the sentiment140 dataset and data file name\n",
    "data_url = 'http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip'\n",
    "data_filename = 'training.1600000.processed.noemoticon.csv'\n",
    "\n",
    "# Column names of the data\n",
    "cols = ['sentiment', 'id', 'date', 'query_string', 'user', 'text']\n",
    "\n",
    "# Data directory\n",
    "data_dir = os.path.join('.', 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(download_url, filedir='.', filename='downloaded_data.zip'):\n",
    "    \"\"\"Download and extract data\"\"\"\n",
    "    if not os.path.isdir(filedir):\n",
    "        os.mkdir(filedir)\n",
    "    downloaded_filename = os.path.join(filedir, filename)\n",
    "    print ('Step 1: Downloading data')\n",
    "    urllib.request.urlretrieve(download_url, downloaded_filename)\n",
    "    print ('Step 2: Extracting data')\n",
    "    zipfile = ZipFile(downloaded_filename)\n",
    "    zipfile.extractall(filedir)\n",
    "    zipfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Downloading data\n",
      "Step 2: Extracting data\n"
     ]
    }
   ],
   "source": [
    "download_data(data_url, filedir=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filepath = os.path.join(data_dir, data_filename)\n",
    "df = pd.read_csv(data_filepath, header=None, names=cols, encoding='iso-8859-1')\n",
    "#df = pd.read_csv(data_filepath, header=None, names=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query_string</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id                          date query_string  \\\n",
       "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009     NO_QUERY   \n",
       "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009     NO_QUERY   \n",
       "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009     NO_QUERY   \n",
       "3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009     NO_QUERY   \n",
       "4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009     NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Prefix for Table Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing split 0\n",
      "processing split 1\n",
      "processing split 2\n",
      "processing split 3\n",
      "processing split 4\n",
      "processing split 5\n",
      "processing split 6\n",
      "processing split 7\n",
      "processing split 8\n",
      "processing split 9\n",
      "processing split 10\n",
      "processing split 11\n",
      "processing split 12\n",
      "processing split 13\n",
      "processing split 14\n",
      "processing split 15\n",
      "processing split 16\n",
      "processing split 17\n",
      "processing split 18\n",
      "processing split 19\n",
      "processing split 20\n",
      "processing split 21\n",
      "processing split 22\n",
      "processing split 23\n",
      "processing split 24\n",
      "processing split 25\n",
      "processing split 26\n",
      "processing split 27\n",
      "processing split 28\n",
      "processing split 29\n",
      "processing split 30\n",
      "processing split 31\n",
      "processing split 32\n",
      "processing split 33\n",
      "processing split 34\n",
      "processing split 35\n",
      "processing split 36\n",
      "processing split 37\n",
      "processing split 38\n",
      "processing split 39\n",
      "processing split 40\n",
      "processing split 41\n",
      "processing split 42\n",
      "processing split 43\n",
      "processing split 44\n",
      "processing split 45\n",
      "processing split 46\n",
      "processing split 47\n",
      "processing split 48\n",
      "processing split 49\n",
      "processing split 50\n",
      "processing split 51\n",
      "processing split 52\n",
      "processing split 53\n",
      "processing split 54\n",
      "processing split 55\n",
      "processing split 56\n",
      "processing split 57\n",
      "processing split 58\n",
      "processing split 59\n",
      "processing split 60\n",
      "processing split 61\n",
      "processing split 62\n",
      "processing split 63\n",
      "processing split 64\n",
      "processing split 65\n",
      "processing split 66\n",
      "processing split 67\n",
      "processing split 68\n",
      "processing split 69\n",
      "processing split 70\n",
      "processing split 71\n",
      "processing split 72\n",
      "processing split 73\n",
      "processing split 74\n",
      "processing split 75\n",
      "processing split 76\n",
      "processing split 77\n",
      "processing split 78\n",
      "processing split 79\n",
      "processing split 80\n",
      "processing split 81\n",
      "processing split 82\n",
      "processing split 83\n",
      "processing split 84\n",
      "processing split 85\n",
      "processing split 86\n",
      "processing split 87\n",
      "processing split 88\n",
      "processing split 89\n",
      "processing split 90\n",
      "processing split 91\n",
      "processing split 92\n",
      "processing split 93\n",
      "processing split 94\n",
      "processing split 95\n",
      "processing split 96\n",
      "processing split 97\n",
      "processing split 98\n",
      "processing split 99\n",
      "processing split 100\n",
      "processing split 101\n",
      "processing split 102\n",
      "processing split 103\n",
      "processing split 104\n",
      "processing split 105\n",
      "processing split 106\n",
      "processing split 107\n",
      "processing split 108\n",
      "processing split 109\n",
      "processing split 110\n",
      "processing split 111\n",
      "processing split 112\n",
      "processing split 113\n",
      "processing split 114\n",
      "processing split 115\n",
      "processing split 116\n",
      "processing split 117\n",
      "processing split 118\n",
      "processing split 119\n",
      "processing split 120\n",
      "processing split 121\n",
      "processing split 122\n",
      "processing split 123\n",
      "processing split 124\n",
      "processing split 125\n",
      "processing split 126\n",
      "processing split 127\n",
      "processing split 128\n",
      "processing split 129\n",
      "processing split 130\n",
      "processing split 131\n",
      "processing split 132\n",
      "processing split 133\n",
      "processing split 134\n",
      "processing split 135\n",
      "processing split 136\n",
      "processing split 137\n",
      "processing split 138\n",
      "processing split 139\n",
      "processing split 140\n",
      "processing split 141\n",
      "processing split 142\n",
      "processing split 143\n",
      "processing split 144\n",
      "processing split 145\n",
      "processing split 146\n",
      "processing split 147\n",
      "processing split 148\n",
      "processing split 149\n",
      "processing split 150\n",
      "processing split 151\n",
      "processing split 152\n",
      "processing split 153\n",
      "processing split 154\n",
      "processing split 155\n",
      "processing split 156\n",
      "processing split 157\n",
      "processing split 158\n",
      "processing split 159\n",
      "processing split 160\n",
      "processing split 161\n",
      "processing split 162\n",
      "processing split 163\n",
      "processing split 164\n",
      "processing split 165\n",
      "processing split 166\n",
      "processing split 167\n",
      "processing split 168\n",
      "processing split 169\n",
      "processing split 170\n",
      "processing split 171\n",
      "processing split 172\n",
      "processing split 173\n",
      "processing split 174\n",
      "processing split 175\n",
      "processing split 176\n",
      "processing split 177\n",
      "processing split 178\n",
      "processing split 179\n",
      "processing split 180\n",
      "processing split 181\n",
      "processing split 182\n",
      "processing split 183\n",
      "processing split 184\n",
      "processing split 185\n",
      "processing split 186\n",
      "processing split 187\n",
      "processing split 188\n",
      "processing split 189\n",
      "processing split 190\n",
      "processing split 191\n",
      "processing split 192\n",
      "processing split 193\n",
      "processing split 194\n",
      "processing split 195\n",
      "processing split 196\n",
      "processing split 197\n",
      "processing split 198\n",
      "processing split 199\n",
      "processing split 200\n",
      "processing split 201\n",
      "processing split 202\n",
      "processing split 203\n",
      "processing split 204\n",
      "processing split 205\n",
      "processing split 206\n",
      "processing split 207\n",
      "processing split 208\n",
      "processing split 209\n",
      "processing split 210\n",
      "processing split 211\n",
      "processing split 212\n",
      "processing split 213\n",
      "processing split 214\n",
      "processing split 215\n",
      "processing split 216\n",
      "processing split 217\n",
      "processing split 218\n",
      "processing split 219\n",
      "processing split 220\n",
      "processing split 221\n",
      "processing split 222\n",
      "processing split 223\n",
      "processing split 224\n",
      "processing split 225\n",
      "processing split 226\n",
      "processing split 227\n",
      "processing split 228\n",
      "processing split 229\n",
      "processing split 230\n",
      "processing split 231\n",
      "processing split 232\n",
      "processing split 233\n",
      "processing split 234\n",
      "processing split 235\n",
      "processing split 236\n",
      "processing split 237\n",
      "processing split 238\n",
      "processing split 239\n",
      "processing split 240\n",
      "processing split 241\n",
      "processing split 242\n",
      "processing split 243\n",
      "processing split 244\n",
      "processing split 245\n",
      "processing split 246\n",
      "processing split 247\n",
      "processing split 248\n",
      "processing split 249\n",
      "processing split 250\n",
      "processing split 251\n",
      "processing split 252\n",
      "processing split 253\n",
      "processing split 254\n",
      "processing split 255\n",
      "processing split 256\n",
      "processing split 257\n",
      "processing split 258\n",
      "processing split 259\n",
      "processing split 260\n",
      "processing split 261\n",
      "processing split 262\n",
      "processing split 263\n",
      "processing split 264\n",
      "processing split 265\n",
      "processing split 266\n",
      "processing split 267\n",
      "processing split 268\n",
      "processing split 269\n",
      "processing split 270\n",
      "processing split 271\n",
      "processing split 272\n",
      "processing split 273\n",
      "processing split 274\n",
      "processing split 275\n",
      "processing split 276\n",
      "processing split 277\n",
      "processing split 278\n",
      "processing split 279\n",
      "processing split 280\n",
      "processing split 281\n",
      "processing split 282\n",
      "processing split 283\n",
      "processing split 284\n",
      "processing split 285\n",
      "processing split 286\n",
      "processing split 287\n",
      "processing split 288\n",
      "processing split 289\n",
      "processing split 290\n",
      "processing split 291\n",
      "processing split 292\n",
      "processing split 293\n",
      "processing split 294\n",
      "processing split 295\n",
      "processing split 296\n",
      "processing split 297\n",
      "processing split 298\n",
      "processing split 299\n",
      "processing split 300\n",
      "processing split 301\n",
      "processing split 302\n",
      "processing split 303\n",
      "processing split 304\n",
      "processing split 305\n",
      "processing split 306\n",
      "processing split 307\n",
      "processing split 308\n",
      "processing split 309\n",
      "processing split 310\n",
      "processing split 311\n",
      "processing split 312\n",
      "processing split 313\n",
      "processing split 314\n",
      "processing split 315\n",
      "processing split 316\n",
      "processing split 317\n",
      "processing split 318\n",
      "processing split 319\n",
      "processing split 320\n",
      "processing split 321\n",
      "processing split 322\n",
      "processing split 323\n",
      "processing split 324\n",
      "processing split 325\n",
      "processing split 326\n",
      "processing split 327\n",
      "processing split 328\n",
      "processing split 329\n",
      "processing split 330\n",
      "processing split 331\n",
      "processing split 332\n",
      "processing split 333\n",
      "processing split 334\n",
      "processing split 335\n",
      "processing split 336\n",
      "processing split 337\n",
      "processing split 338\n",
      "processing split 339\n",
      "processing split 340\n",
      "processing split 341\n",
      "processing split 342\n",
      "processing split 343\n",
      "processing split 344\n",
      "processing split 345\n",
      "processing split 346\n",
      "processing split 347\n",
      "processing split 348\n",
      "processing split 349\n",
      "processing split 350\n",
      "processing split 351\n",
      "processing split 352\n",
      "processing split 353\n",
      "processing split 354\n",
      "processing split 355\n",
      "processing split 356\n",
      "processing split 357\n",
      "processing split 358\n",
      "processing split 359\n",
      "processing split 360\n",
      "processing split 361\n",
      "processing split 362\n",
      "processing split 363\n",
      "processing split 364\n",
      "processing split 365\n",
      "processing split 366\n",
      "processing split 367\n",
      "processing split 368\n",
      "processing split 369\n",
      "processing split 370\n",
      "processing split 371\n",
      "processing split 372\n",
      "processing split 373\n",
      "processing split 374\n",
      "processing split 375\n",
      "processing split 376\n",
      "processing split 377\n",
      "processing split 378\n",
      "processing split 379\n",
      "processing split 380\n",
      "processing split 381\n",
      "processing split 382\n",
      "processing split 383\n",
      "processing split 384\n",
      "processing split 385\n",
      "processing split 386\n",
      "processing split 387\n",
      "processing split 388\n",
      "processing split 389\n",
      "processing split 390\n",
      "processing split 391\n",
      "processing split 392\n",
      "processing split 393\n",
      "processing split 394\n",
      "processing split 395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing split 396\n",
      "processing split 397\n",
      "processing split 398\n",
      "processing split 399\n",
      "processing split 400\n",
      "processing split 401\n",
      "processing split 402\n",
      "processing split 403\n",
      "processing split 404\n",
      "processing split 405\n",
      "processing split 406\n",
      "processing split 407\n",
      "processing split 408\n",
      "processing split 409\n",
      "processing split 410\n",
      "processing split 411\n",
      "processing split 412\n",
      "processing split 413\n",
      "processing split 414\n",
      "processing split 415\n",
      "processing split 416\n",
      "processing split 417\n",
      "processing split 418\n",
      "processing split 419\n",
      "processing split 420\n",
      "processing split 421\n",
      "processing split 422\n",
      "processing split 423\n",
      "processing split 424\n",
      "processing split 425\n",
      "processing split 426\n",
      "processing split 427\n",
      "processing split 428\n",
      "processing split 429\n",
      "processing split 430\n",
      "processing split 431\n",
      "processing split 432\n",
      "processing split 433\n",
      "processing split 434\n",
      "processing split 435\n",
      "processing split 436\n",
      "processing split 437\n",
      "processing split 438\n",
      "processing split 439\n",
      "processing split 440\n",
      "processing split 441\n",
      "processing split 442\n",
      "processing split 443\n",
      "processing split 444\n",
      "processing split 445\n",
      "processing split 446\n",
      "processing split 447\n",
      "processing split 448\n",
      "processing split 449\n",
      "processing split 450\n",
      "processing split 451\n",
      "processing split 452\n",
      "processing split 453\n",
      "processing split 454\n",
      "processing split 455\n",
      "processing split 456\n",
      "processing split 457\n",
      "processing split 458\n",
      "processing split 459\n",
      "processing split 460\n",
      "processing split 461\n",
      "processing split 462\n",
      "processing split 463\n",
      "processing split 464\n",
      "processing split 465\n",
      "processing split 466\n",
      "processing split 467\n",
      "processing split 468\n",
      "processing split 469\n",
      "processing split 470\n",
      "processing split 471\n",
      "processing split 472\n",
      "processing split 473\n",
      "processing split 474\n",
      "processing split 475\n",
      "processing split 476\n",
      "processing split 477\n",
      "processing split 478\n",
      "processing split 479\n",
      "processing split 480\n",
      "processing split 481\n",
      "processing split 482\n",
      "processing split 483\n",
      "processing split 484\n",
      "processing split 485\n",
      "processing split 486\n",
      "processing split 487\n",
      "processing split 488\n",
      "processing split 489\n",
      "processing split 490\n",
      "processing split 491\n",
      "processing split 492\n",
      "processing split 493\n",
      "processing split 494\n",
      "processing split 495\n",
      "processing split 496\n",
      "processing split 497\n",
      "processing split 498\n",
      "processing split 499\n",
      "processing split 500\n",
      "processing split 501\n",
      "processing split 502\n",
      "processing split 503\n",
      "processing split 504\n",
      "processing split 505\n",
      "processing split 506\n",
      "processing split 507\n",
      "processing split 508\n",
      "processing split 509\n",
      "processing split 510\n",
      "processing split 511\n"
     ]
    }
   ],
   "source": [
    "# Number of splits\n",
    "n_splits = 512\n",
    "n_digits = len(str(n_splits)) + 1\n",
    "\n",
    "bucket_size = df.shape[0] // n_splits\n",
    "for i in range(n_splits):\n",
    "    print('processing split {}'.format(i))\n",
    "    start = i*bucket_size\n",
    "    if i < n_splits-1: \n",
    "        end = (i+1)*bucket_size\n",
    "    else:\n",
    "        end = df.shape[0]\n",
    "    idx_range = range(start, end)\n",
    "    prefix = str(str(i).zfill(n_digits))\n",
    "    df.loc[idx_range, 'id'] = prefix + '_' + df.loc[idx_range, 'id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to file\n",
    "df.to_csv(os.path.join(data_dir, 'sentiment140_prefix.csv'), header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prefix = pd.read_csv(os.path.join(data_dir, 'sentiment140_prefix.csv'), header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0000_1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0000_1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0000_1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0000_1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0000_1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599995</td>\n",
       "      <td>4</td>\n",
       "      <td>0511_2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599996</td>\n",
       "      <td>4</td>\n",
       "      <td>0511_2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599997</td>\n",
       "      <td>4</td>\n",
       "      <td>0511_2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599998</td>\n",
       "      <td>4</td>\n",
       "      <td>0511_2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599999</td>\n",
       "      <td>4</td>\n",
       "      <td>0511_2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0                1                             2         3  \\\n",
       "0        0  0000_1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1        0  0000_1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2        0  0000_1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3        0  0000_1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4        0  0000_1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "...     ..              ...                           ...       ...   \n",
       "1599995  4  0511_2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996  4  0511_2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997  4  0511_2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998  4  0511_2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599999  4  0511_2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                       4                                                  5  \n",
       "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1          scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2               mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3                ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4                 Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "...                  ...                                                ...  \n",
       "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
       "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
       "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
       "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
       "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
       "\n",
       "[1600000 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prefix.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
